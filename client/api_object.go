package client

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/hex"
	"encoding/json"
	"encoding/xml"
	"errors"
	"fmt"
	"io"
	"math"
	"net/http"
	"net/url"
	"os"
	"strconv"
	"strings"
	"sync/atomic"
	"time"

	hashlib "github.com/bnb-chain/greenfield-common/go/hash"
	"github.com/bnb-chain/greenfield-common/go/redundancy"
	"github.com/bnb-chain/greenfield-go-sdk/pkg/utils"
	"github.com/bnb-chain/greenfield-go-sdk/types"
	gnfdsdk "github.com/bnb-chain/greenfield/sdk/types"
	gnfdTypes "github.com/bnb-chain/greenfield/types"
	"github.com/bnb-chain/greenfield/types/s3util"
	permTypes "github.com/bnb-chain/greenfield/x/permission/types"
	storageTypes "github.com/bnb-chain/greenfield/x/storage/types"
	sdk "github.com/cosmos/cosmos-sdk/types"
	"github.com/cosmos/cosmos-sdk/types/tx"
	"github.com/rs/zerolog/log"
)

type Object interface {
	GetCreateObjectApproval(ctx context.Context, createObjectMsg *storageTypes.MsgCreateObject) (*storageTypes.MsgCreateObject, error)
	CreateObject(ctx context.Context, bucketName, objectName string, reader io.Reader, opts types.CreateObjectOptions) (string, error)
	PutObject(ctx context.Context, bucketName, objectName string, objectSize int64, reader io.Reader, opts types.PutObjectOptions) error
	putpObjectResumable(ctx context.Context, bucketName, objectName string, objectSize int64, reader io.Reader, opts types.PutObjectOptions) error
	FPutObject(ctx context.Context, bucketName, objectName, filePath string, opts types.PutObjectOptions) (err error)
	CancelCreateObject(ctx context.Context, bucketName, objectName string, opt types.CancelCreateOption) (string, error)
	DeleteObject(ctx context.Context, bucketName, objectName string, opt types.DeleteObjectOption) (string, error)
	GetObject(ctx context.Context, bucketName, objectName string, opts types.GetObjectOptions) (io.ReadCloser, types.ObjectStat, error)
	FGetObject(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error

	// HeadObject query the objectInfo on chain to check th object id, return the object info if exists
	// return err info if object not exist
	HeadObject(ctx context.Context, bucketName, objectName string) (*storageTypes.ObjectInfo, error)
	// HeadObjectByID query the objectInfo on chain by object id, return the object info if exists
	// return err info if object not exist
	HeadObjectByID(ctx context.Context, objID string) (*storageTypes.ObjectInfo, error)
	// UpdateObjectVisibility update the visibility of the object
	UpdateObjectVisibility(ctx context.Context, bucketName, objectName string, visibility storageTypes.VisibilityType, opt types.UpdateObjectOption) (string, error)
	// PutObjectPolicy apply object policy to the principal, return the txn hash
	// The principal can be generated by NewPrincipalWithAccount or NewPrincipalWithGroupId
	PutObjectPolicy(ctx context.Context, bucketName, objectName string, principal types.Principal,
		statements []*permTypes.Statement, opt types.PutPolicyOption) (string, error)
	// DeleteObjectPolicy delete the object policy of the principal, return the txn hash
	// The principal can be generated by NewPrincipalWithAccount or NewPrincipalWithGroupId
	DeleteObjectPolicy(ctx context.Context, bucketName, objectName string, principal types.Principal, opt types.DeletePolicyOption) (string, error)
	// GetObjectPolicy get the object policy info of the user specified by principalAddr.
	// principalAddr indicates the HEX-encoded string of the principal address
	GetObjectPolicy(ctx context.Context, bucketName, objectName string, principalAddr string) (*permTypes.Policy, error)
	// IsObjectPermissionAllowed check if the permission of the object is allowed to the user
	// userAddr indicates the HEX-encoded string of the user address
	IsObjectPermissionAllowed(ctx context.Context, userAddr string, bucketName, objectName string, action permTypes.ActionType) (permTypes.Effect, error)
	ListObjects(ctx context.Context, bucketName string, opts types.ListObjectsOptions) (types.ListObjectsResult, error)
	// ComputeHashRoots compute the integrity hash, content size and the redundancy type of the file
	ComputeHashRoots(reader io.Reader) ([][]byte, int64, storageTypes.RedundancyType, error)

	// CreateFolder creates an empty object used as folder.
	// objectName must ending with a forward slash (/) character
	CreateFolder(ctx context.Context, bucketName, objectName string, opts types.CreateObjectOptions) (string, error)
	// GetObjectUploadProgress return the status of the uploading object
	GetObjectUploadProgress(ctx context.Context, bucketName, objectName string) (string, error)

	// RecoverObjectBySecondary get piece data from secondary SPs and recovery the object in client
	RecoverObjectBySecondary(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error
	// GetObjectResumableUploadOffset return the status of the uploading object
	GetObjectResumableUploadOffset(ctx context.Context, bucketName, objectName string) (uint64, error)
}

// GetRedundancyParams query and return the data shards, parity shards and segment size of redundancy
// configuration on chain
func (c *client) GetRedundancyParams() (uint32, uint32, uint64, error) {
	query := storageTypes.QueryParamsRequest{}
	queryResp, err := c.chainClient.StorageQueryClient.Params(context.Background(), &query)
	if err != nil {
		return 0, 0, 0, err
	}

	versionedParams := queryResp.Params.VersionedParams
	return versionedParams.GetRedundantDataChunkNum(), versionedParams.GetRedundantParityChunkNum(), versionedParams.GetMaxSegmentSize(), nil
}

// GetParams query and return the data shards, parity shards and segment size of redundancy
// configuration on chain
func (c *client) GetParams() (storageTypes.Params, error) {
	query := storageTypes.QueryParamsRequest{}
	queryResp, err := c.chainClient.StorageQueryClient.Params(context.Background(), &query)
	if err != nil {
		return storageTypes.Params{}, err
	}

	return queryResp.Params, nil
}

// ComputeHashRoots return the integrity hash, content size and the redundancy type of the file
func (c *client) ComputeHashRoots(reader io.Reader) ([][]byte, int64, storageTypes.RedundancyType, error) {
	dataBlocks, parityBlocks, segSize, err := c.GetRedundancyParams()
	if reader == nil {
		return nil, 0, storageTypes.REDUNDANCY_EC_TYPE, errors.New("fail to compute hash, reader is nil")
	}
	if err != nil {
		return nil, 0, storageTypes.REDUNDANCY_EC_TYPE, err
	}

	return hashlib.ComputeIntegrityHash(reader, int64(segSize), int(dataBlocks), int(parityBlocks))
}

// CreateObject get approval of creating object and send createObject txn to greenfield chain,
// it returns the transaction hash value and error
func (c *client) CreateObject(ctx context.Context, bucketName, objectName string,
	reader io.Reader, opts types.CreateObjectOptions,
) (string, error) {
	if reader == nil {
		return "", errors.New("fail to compute hash of payload, reader is nil")
	}

	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	// compute hash root of payload
	expectCheckSums, size, redundancyType, err := c.ComputeHashRoots(reader)
	if err != nil {
		return "", err
	}

	var contentType string
	if opts.ContentType != "" {
		contentType = opts.ContentType
	} else {
		contentType = types.ContentDefault
	}

	var visibility storageTypes.VisibilityType
	if opts.Visibility == storageTypes.VISIBILITY_TYPE_UNSPECIFIED {
		visibility = storageTypes.VISIBILITY_TYPE_INHERIT // set default visibility type
	} else {
		visibility = opts.Visibility
	}

	createObjectMsg := storageTypes.NewMsgCreateObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName,
		uint64(size), visibility, expectCheckSums, contentType, redundancyType, math.MaxUint, nil, opts.SecondarySPAccs)
	err = createObjectMsg.ValidateBasic()
	if err != nil {
		return "", err
	}

	signedCreateObjectMsg, err := c.GetCreateObjectApproval(ctx, createObjectMsg)
	if err != nil {
		return "", err
	}

	// set the default txn broadcast mode as block mode
	if opts.TxOpts == nil {
		broadcastMode := tx.BroadcastMode_BROADCAST_MODE_SYNC
		opts.TxOpts = &gnfdsdk.TxOption{Mode: &broadcastMode}
	}

	resp, err := c.chainClient.BroadcastTx(ctx, []sdk.Msg{signedCreateObjectMsg}, opts.TxOpts)
	if err != nil {
		return "", err
	}

	txnHash := resp.TxResponse.TxHash
	var txnResponse *sdk.TxResponse
	if !opts.IsAsyncMode {
		ctxTimeout, cancel := context.WithTimeout(ctx, types.ContextTimeout)
		defer cancel()
		txnResponse, err = c.WaitForTx(ctxTimeout, txnHash)
		if err != nil {
			return txnHash, fmt.Errorf("the transaction has been submitted, please check it later:%v", err)
		}
		if txnResponse.Code != 0 {
			return txnHash, fmt.Errorf("the createObject txn has failed with response code: %d", txnResponse.Code)
		}
	}
	return txnHash, nil
}

// DeleteObject send DeleteBucket txn to greenfield chain and return txn hash
func (c *client) DeleteObject(ctx context.Context, bucketName, objectName string, opt types.DeleteObjectOption) (string, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	delObjectMsg := storageTypes.NewMsgDeleteObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName)
	return c.sendTxn(ctx, delObjectMsg, opt.TxOpts)
}

// CancelCreateObject send CancelCreateObject txn to greenfield chain
func (c *client) CancelCreateObject(ctx context.Context, bucketName, objectName string, opt types.CancelCreateOption) (string, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	cancelCreateMsg := storageTypes.NewMsgCancelCreateObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName)
	return c.sendTxn(ctx, cancelCreateMsg, opt.TxOpts)
}

// PutObject supports the second stage of uploading the object to bucket.
// txnHash should be the str which hex.encoding from txn hash bytes
func (c *client) PutObject(ctx context.Context, bucketName, objectName string, objectSize int64,
	reader io.Reader, opts types.PutObjectOptions,
) (err error) {
	if objectSize <= 0 {
		return errors.New("object size should be more than 0")
	}

	// minPartSize: 16MB
	partSize := opts.PartSize
	if opts.PartSize == 0 {
		partSize = types.MinPartSize
	}

	if objectSize >= 0 && objectSize < int64(partSize) || opts.DisableResumable {
		return c.putObject(ctx, bucketName, objectName, objectSize, reader, opts)
	}

	return c.putpObjectResumable(ctx, bucketName, objectName, objectSize, reader, opts)
}

func (c *client) putObject(ctx context.Context, bucketName, objectName string, objectSize int64,
	reader io.Reader, opts types.PutObjectOptions,
) (err error) {
	if objectSize <= 0 {
		return errors.New("object size should be more than 0")
	}

	if err := c.headSPObjectInfo(ctx, bucketName, objectName); err != nil {
		log.Error().Msg(fmt.Sprintf("fail to head object %s , err %v ", objectName, err))
		return err
	}

	var contentType string
	if opts.ContentType != "" {
		contentType = opts.ContentType
	} else {
		contentType = types.ContentDefault
	}

	reqMeta := requestMeta{
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
		contentLength: objectSize,
		contentType:   contentType,
	}

	var sendOpt sendOptions
	if opts.TxnHash != "" {
		sendOpt = sendOptions{
			method:  http.MethodPut,
			body:    reader,
			txnHash: opts.TxnHash,
		}
	} else {
		sendOpt = sendOptions{
			method: http.MethodPut,
			body:   reader,
		}
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return err
	}

	_, err = c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return err
	}

	return nil
}

//// UploadSegment defines upload Segment
//type UploadSegment struct {
//	Index  int    // Segment number, starting from 0
//	Start  int64  // Start index
//	End    int64  // End index
//	Offset int64  // Offset
//	CRC64  uint64 // CRC check value of Segment
//}

// UploadSegmentHook is for testing usage
type uploadSegmentHook func(id int) error

var UploadSegmentHooker uploadSegmentHook = DefaultUploadSegment

func DefaultUploadSegment(id int) error {
	return nil
}

func (c *client) putpObjectResumable(ctx context.Context, bucketName, objectName string, objectSize int64,
	reader io.Reader, opts types.PutObjectOptions,
) (err error) {

	if err := c.headSPObjectInfo(ctx, bucketName, objectName); err != nil {
		log.Error().Msg(fmt.Sprintf("fail to head object %s , err %v ", objectName, err))
		return err
	}

	offset, err := c.GetObjectResumableUploadOffset(ctx, bucketName, objectName)
	if err != nil {
		return err
	}

	// Total data read and written to server. should be equal to
	// 'size' at the end of the call.
	var totalUploadedSize int64

	// Calculate the optimal parts info for a given size.
	totalPartsCount, partSize, _, err := c.OptimalPartInfo(objectSize, opts.PartSize)
	if err != nil {
		return err
	}

	// Part number always starts with '1'.
	partNumber := 1
	startPartNumber := int(offset/opts.PartSize + 1)

	// Create a buffer.
	buf := make([]byte, partSize)
	complete := false

	// skip sucess segment TODO(chris): seek ?
	for partNumber < startPartNumber {
		length, rErr := utils.ReadFull(reader, buf)
		if rErr == io.EOF && partNumber > 1 {
			break
		}
		// Increment part number.
		log.Info().Msg(fmt.Sprintf("skip partNumber:%d, length:%d", partNumber, length))
		// Save successfully uploaded size.
		totalUploadedSize += int64(length)
		partNumber++
	}

	for partNumber <= totalPartsCount {
		if partNumber == totalPartsCount {
			complete = true
		}
		// hook for test
		if err = UploadSegmentHooker(partNumber); err != nil {
			return err
		}
		length, rErr := utils.ReadFull(reader, buf)
		if rErr == io.EOF && partNumber > 1 {
			break
		}

		if rErr != nil && rErr != io.ErrUnexpectedEOF && rErr != io.EOF {
			return err
		}

		log.Info().Msg(fmt.Sprintf("partNumber:%d, length:%d", partNumber, length))

		// Update progress reader appropriately to the latest offset
		// as we read from the source.
		rd := bytes.NewReader(buf[:length])

		var contentType string
		if opts.ContentType != "" {
			contentType = opts.ContentType
		} else {
			contentType = types.ContentDefault
		}

		// Initialize url queries.
		urlValues := make(url.Values)
		urlValues.Set("offset", strconv.FormatInt(totalUploadedSize, 10))
		urlValues.Set("complete", strconv.FormatBool(complete))
		urlValues.Set("context", "")

		reqMeta := requestMeta{
			bucketName:    bucketName,
			objectName:    objectName,
			contentSHA256: types.EmptyStringSHA256,
			contentLength: int64(length),
			contentType:   contentType,
			urlValues:     urlValues,
		}

		var sendOpt sendOptions
		if opts.TxnHash != "" {
			sendOpt = sendOptions{
				method:  http.MethodPost,
				body:    rd,
				txnHash: opts.TxnHash,
			}
		} else {
			sendOpt = sendOptions{
				method: http.MethodPost,
				body:   rd,
			}
		}

		endpoint, err := c.getSPUrlByBucket(bucketName)
		if err != nil {
			log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
			return err
		}

		// Proceed to upload the part.
		_, err = c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
		if err != nil {
			return err
		}

		// Save successfully uploaded size.
		totalUploadedSize += int64(length)

		// Increment part number.
		partNumber++

		// For unknown size, Read EOF we break away.
		// We do not have to upload till totalPartsCount.
		if rErr == io.EOF {
			break
		}
	}

	return nil
}

func (c *client) headSPObjectInfo(ctx context.Context, bucketName, objectName string) error {
	backoffDelay := types.HeadBackOffDelay
	for retry := 0; retry < types.MaxHeadTryTime; retry++ {
		_, err := c.getObjectStatusFromSP(ctx, bucketName, objectName)
		if err == nil {
			return nil
		}
		// if the error is not "no such object", ignore it
		if !strings.Contains(strings.ToLower(err.Error()), types.NoSuchObjectErr) {
			return nil
		}

		if retry == types.MaxHeadTryTime-1 {
			return fmt.Errorf(" sp failed to head info of the object: %s, please try putObject later", objectName)
		}

		time.Sleep(backoffDelay)
		backoffDelay *= 2
	}

	return nil
}

// FPutObject supports uploading object from local file
func (c *client) FPutObject(ctx context.Context, bucketName, objectName, filePath string, opts types.PutObjectOptions) (err error) {
	fReader, err := os.Open(filePath)
	// If any error fail quickly here.
	if err != nil {
		return err
	}
	defer fReader.Close()

	// Save the file stat.
	stat, err := fReader.Stat()
	if err != nil {
		return err
	}

	return c.PutObject(ctx, bucketName, objectName, stat.Size(), fReader, opts)
}

// GetObject download s3 object payload and return the related object info
func (c *client) GetObject(ctx context.Context, bucketName, objectName string,
	opts types.GetObjectOptions,
) (io.ReadCloser, types.ObjectStat, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return nil, types.ObjectStat{}, err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return nil, types.ObjectStat{}, err
	}

	reqMeta := requestMeta{
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
	}

	if opts.Range != "" {
		reqMeta.rangeInfo = opts.Range
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed,  err: %s", bucketName, err.Error()))
		return nil, types.ObjectStat{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return nil, types.ObjectStat{}, err
	}

	objStat, err := getObjInfo(objectName, resp.Header)
	if err != nil {
		utils.CloseResponse(resp)
		return nil, types.ObjectStat{}, err
	}

	return resp.Body, objStat, nil
}

// FGetObject download s3 object payload adn write the object content into local file specified by filePath
func (c *client) FGetObject(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error {
	// Verify if destination already exists.
	st, err := os.Stat(filePath)
	if err == nil {
		// If the destination exists and is a directory.
		if st.IsDir() {
			return errors.New("download file path is a directory")
		}
		return errors.New("download file already exist")
	}

	backoffDelay := types.DownloadBackOffDelay
	var body io.ReadCloser
	for retry := 0; retry < types.MaxDownloadTryTime; retry++ {
		body, _, err = c.GetObject(ctx, bucketName, objectName, opts)
		if err == nil {
			break
		}
		body.Close()

		connectedFail := strings.Contains(err.Error(), types.GetConnectionFail)
		if err != nil && !connectedFail {
			return err
		}

		// connect the primary SP failed, try again
		if opts.SupportRecovery {
			// connect failed for 3 times, try to download piece from secondary SP
			if retry == types.MaxDownloadTryTime-1 {
				return c.RecoverObjectBySecondary(ctx, bucketName, objectName, filePath, opts)
			}
			continue
		}

		time.Sleep(backoffDelay)
		backoffDelay *= 2
	}

	defer body.Close()

	fd, err := os.OpenFile(filePath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0o660)
	if err != nil {
		return err
	}

	_, err = io.Copy(fd, body)
	fd.Close()
	if err != nil {
		return err
	}

	return nil
}

func (c *client) RecoverObjectBySecondary(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error {
	// compute segment count
	dataBlocks, parityBlocks, maxSegmentSize, err := c.GetRedundancyParams()
	if err != nil {
		return err
	}
	ecPieceCount := dataBlocks + parityBlocks
	minRecoveryPieces := dataBlocks

	objectInfo, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return err
	}
	objectSize := objectInfo.PayloadSize
	startSegmentIdx, endSegmentIdx, diffStartOffset, diffEndOffset, err := checkGetObjectRange(objectSize, maxSegmentSize, opts)
	if err != nil {
		return err
	}

	secondaryEndpoints, err := c.getSecondaryEndpoints(ctx, objectInfo)
	if err != nil {
		return err
	}

	_, err = os.Stat(filePath)
	if err == nil {
		return errors.New("download file already exist:" + filePath)
	}

	f, err := os.OpenFile(filePath, os.O_APPEND|os.O_WRONLY|os.O_CREATE, 0644)
	if err != nil {
		return err
	}
	defer f.Close()

	// iterate over segment indices and recover one by one
	for segmentIdx := startSegmentIdx; segmentIdx <= endSegmentIdx; segmentIdx++ {
		doneCh := make(chan bool, len(secondaryEndpoints))
		quitCh := make(chan bool)
		totalTaskNum := int32(ecPieceCount)
		doneTaskNum := uint32(0)
		downLoadPieceSize := 0
		var recoveryDataSources = make([][]byte, ecPieceCount)

		segmentSize := utils.GetSegmentSize(objectSize, uint32(segmentIdx), maxSegmentSize)
		for ecIdx := 0; ecIdx < int(ecPieceCount); ecIdx++ {
			recoveryDataSources[ecIdx] = nil
			go func(secondaryIndex int) {
				var responseBody io.ReadCloser
				var pieceData []byte
				defer func() {
					// fmt.Printf("get done routine: %d \n", atomic.LoadUint32(&ecPieceCount))
					// finish all the task, send signal to quitCh
					if atomic.AddInt32(&totalTaskNum, -1) == 0 {
						quitCh <- true
						downLoadPieceSize = len(pieceData)
					}
					if responseBody != nil {
						responseBody.Close()
					}
				}()
				pieceInfo := types.QueryPieceInfo{
					ObjectId:        strconv.FormatUint(objectInfo.Id.Uint64(), 10),
					PieceIndex:      segmentIdx,
					RedundancyIndex: secondaryIndex,
				}
				// call getSecondaryPieceData to retrieve recovery data for the segment
				// TODO check if it is better with downloading data chunks first
				responseBody, err = c.getSecondaryPieceData(ctx, bucketName, objectName, pieceInfo, types.GetSecondaryPieceOptions{Endpoint: secondaryEndpoints[secondaryIndex]})
				if err == nil {
					// convert recoveryData to byte
					pieceData, err = io.ReadAll(responseBody)
					if err != nil {
						log.Error().Msg("read body err:" + err.Error())
						return
					}
					recoveryDataSources[secondaryIndex] = pieceData
					doneCh <- true
				} else {
					log.Error().Msg("get piece from secondary SP error:" + err.Error())
				}

			}(ecIdx)
		}

	loop:
		for {
			select {
			case <-doneCh:
				doneTaskNum++
				// it is enough to recovery data with minRecoveryPieces EC data, no need to wait
				if doneTaskNum >= minRecoveryPieces {
					break loop
				}
			case <-quitCh: // all the task finish
				if doneTaskNum < minRecoveryPieces { // finish task num not enough
					return fmt.Errorf("get piece from secondary not enough %d", doneTaskNum)
				}
				ecTotalSize := int64(uint32(downLoadPieceSize) * dataBlocks)
				if ecTotalSize < segmentSize || ecTotalSize > segmentSize+4 {
					return fmt.Errorf("get secondary piece data length error")
				}
			}
		}

		// decode the original segment data from the piece data
		recoverySegData, err := redundancy.DecodeRawSegment(recoveryDataSources, segmentSize, int(dataBlocks), int(parityBlocks))
		if err != nil {
			log.Error().Msg(fmt.Sprintf("decode segment err, segment id:%d err: %s", segmentIdx, err.Error()))
			return fmt.Errorf("decode segment err, segment id:%d err: %s", segmentIdx, err.Error())
		}

		isFirstSeg := segmentIdx == startSegmentIdx && diffStartOffset > 0
		isLastSeg := segmentIdx == endSegmentIdx && diffEndOffset > 0

		// deal with range recovery
		if isFirstSeg && isLastSeg {
			recoverySegData = recoverySegData[diffStartOffset : segmentSize-diffEndOffset]
		} else if isFirstSeg {
			recoverySegData = recoverySegData[diffStartOffset:]
			log.Printf("first segment diff offset %d \n", diffStartOffset)
		} else if isLastSeg {
			recoverySegData = recoverySegData[:segmentSize-diffEndOffset]
			log.Printf("last segment diff offset %d \n", diffEndOffset)
		}

		_, err = f.Write(recoverySegData)
		if err != nil {
			return err
		}
		log.Printf(fmt.Sprintf("finish recovery object:%s segment id:%d ", objectName, segmentIdx))
	}

	return nil
}

func checkGetObjectRange(payloadSize uint64, maxSegmentSize uint64, opts types.GetObjectOptions) (int, int, int64, int64, error) {
	var (
		rangeStart, rangeEnd int64
		diffStartOffset      int64
		diffEndOffset        int64
		startSegmentIdx      int
		endSegmentIdx        int
	)
	segmentCount := utils.GetSegmentCount(payloadSize, maxSegmentSize)
	if opts.Range == "" {
		return 0, int(segmentCount - 1), 0, 0, nil
	}

	isRange, rangeStart, rangeEnd := utils.ParseRange(opts.Range)
	if isRange && (rangeEnd < 0 || rangeEnd >= int64(payloadSize)) {
		rangeEnd = int64(payloadSize) - 1
	}

	if isRange && (rangeStart < 0 || rangeEnd < 0 || rangeStart > rangeEnd) {
		return 0, 0, 0, 0, errors.New("invalid range: " + opts.Range)
	}

	if isRange {
		startSegmentIdx = int(rangeStart / int64(maxSegmentSize))
		diffStartOffset = rangeStart - int64(startSegmentIdx)*int64(maxSegmentSize)

		endSegmentIdx = int(rangeEnd / int64(maxSegmentSize))
		endSegSize := utils.GetSegmentSize(payloadSize, uint32(endSegmentIdx), maxSegmentSize)
		diffEndOffset = int64(endSegmentIdx)*int64(maxSegmentSize) + endSegSize - rangeEnd - 1

		if diffEndOffset > endSegSize {
			log.Error().Msg("compute end segment diff offset error ")
			return 0, 0, 0, 0, errors.New("compute end segment diff offset error ")
		}
	} else {
		startSegmentIdx = 0
		endSegmentIdx = int(segmentCount - 1)
	}

	log.Printf("startSet %d, diffStartOffset %d,  endSegmentIdx %d, diffEndOffset %d,", startSegmentIdx, diffStartOffset, endSegmentIdx, diffEndOffset)
	return startSegmentIdx, endSegmentIdx, diffStartOffset, diffEndOffset, nil
}

func (c *client) getSecondaryEndpoints(ctx context.Context, objectInfo *storageTypes.ObjectInfo) ([]string, error) {
	secondarySPAddrs := objectInfo.GetSecondarySpAddresses()
	spList, err := c.ListStorageProviders(ctx, false)
	if err != nil {
		return nil, err
	}

	secondaryEndpointList := make([]string, len(secondarySPAddrs))
	for idx, addr := range secondarySPAddrs {
		for _, info := range spList {
			if addr == info.GetOperatorAddress() {
				secondaryEndpointList[idx] = info.Endpoint
			}
		}
	}

	fmt.Printf("secondEndpont first: %s, len: %d \n", secondaryEndpointList[0], len(secondaryEndpointList))
	return secondaryEndpointList, nil
}

func (c *client) getSecondaryPieceData(ctx context.Context, bucketName, objectName string, pieceInfo types.QueryPieceInfo, opts types.GetSecondaryPieceOptions) (io.ReadCloser, error) {
	var err error
	dataBlocks, parityBlocks, _, err := c.GetRedundancyParams()
	if err != nil {
		return nil, errors.New("fail to get redundancy params:" + err.Error())
	}
	maxRedundancyIndex := int(dataBlocks+parityBlocks) - 1
	if pieceInfo.RedundancyIndex > maxRedundancyIndex || pieceInfo.RedundancyIndex < 0 {
		return nil, fmt.Errorf("redundancy index invalid, the index should be %d to %d", 0, maxRedundancyIndex)
	}

	params := url.Values{}
	params.Set("get-piece", "")

	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
		pieceInfo:     pieceInfo,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	var endpoint *url.URL
	if opts.Endpoint != "" {
		var useHttps bool
		if strings.Contains(opts.Endpoint, "https") {
			useHttps = true
		} else {
			useHttps = c.secure
		}

		endpoint, err = utils.GetEndpointURL(opts.Endpoint, useHttps)
		if err != nil {
			log.Error().Msg(fmt.Sprintf("fetch endpoint from opts %s fail:%v", opts.Endpoint, err))
			return nil, err
		}
	} else if opts.SPAddress != "" {
		// get endpoint from sp address
		endpoint, err = c.getSPUrlByAddr(opts.SPAddress)
		if err != nil {
			log.Error().Msg(fmt.Sprintf("route endpoint by secondary sp address: %s failed, err: %v", opts.SPAddress, err))
			return nil, err
		}
	} else {
		// get secondary sp address info based on the redundancy index
		objectInfo, err := c.HeadObjectByID(ctx, pieceInfo.ObjectId)
		if err != nil {
			return nil, err
		}

		secondarySP := objectInfo.SecondarySpAddresses[pieceInfo.RedundancyIndex]
		endpoint, err = c.getSPUrlByAddr(secondarySP)
		if err != nil {
			log.Error().Msg(fmt.Sprintf("route endpoint by sp address: %s failed, err: %v", secondarySP, err))
			return nil, err
		}
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return nil, err
	}

	return resp.Body, nil
}

// getObjInfo generates objectInfo base on the response http header content
func getObjInfo(objectName string, h http.Header) (types.ObjectStat, error) {
	// Parse content length is exists
	var size int64 = -1
	var err error
	contentLength := h.Get(types.HTTPHeaderContentLength)
	if contentLength != "" {
		size, err = strconv.ParseInt(contentLength, 10, 64)
		if err != nil {
			return types.ObjectStat{}, types.ErrResponse{
				Code:    "InternalError",
				Message: fmt.Sprintf("Content-Length parse error %v", err),
			}
		}
	}

	// fetch content type
	contentType := strings.TrimSpace(h.Get("Content-Type"))
	if contentType == "" {
		contentType = types.ContentDefault
	}

	return types.ObjectStat{
		ObjectName:  objectName,
		ContentType: contentType,
		Size:        size,
	}, nil
}

// HeadObject query the objectInfo on chain to check th object id, return the object info if exists
// return err info if object not exist
func (c *client) HeadObject(ctx context.Context, bucketName, objectName string) (*storageTypes.ObjectInfo, error) {
	queryHeadObjectRequest := storageTypes.QueryHeadObjectRequest{
		BucketName: bucketName,
		ObjectName: objectName,
	}
	queryHeadObjectResponse, err := c.chainClient.HeadObject(ctx, &queryHeadObjectRequest)
	if err != nil {
		return nil, err
	}

	return queryHeadObjectResponse.ObjectInfo, nil
}

// HeadObjectByID query the objectInfo on chain by object id, return the object info if exists
// return err info if object not exist
func (c *client) HeadObjectByID(ctx context.Context, objID string) (*storageTypes.ObjectInfo, error) {
	headObjectRequest := storageTypes.QueryHeadObjectByIdRequest{
		ObjectId: objID,
	}
	queryHeadObjectResponse, err := c.chainClient.HeadObjectById(ctx, &headObjectRequest)
	if err != nil {
		return nil, err
	}

	return queryHeadObjectResponse.ObjectInfo, nil
}

// PutObjectPolicy apply object policy to the principal, return the txn hash
func (c *client) PutObjectPolicy(ctx context.Context, bucketName, objectName string, principalStr types.Principal,
	statements []*permTypes.Statement, opt types.PutPolicyOption,
) (string, error) {
	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)

	principal := &permTypes.Principal{}
	if err := principal.Unmarshal([]byte(principalStr)); err != nil {
		return "", err
	}

	putPolicyMsg := storageTypes.NewMsgPutPolicy(c.MustGetDefaultAccount().GetAddress(), resource.String(),
		principal, statements, opt.PolicyExpireTime)

	return c.sendPutPolicyTxn(ctx, putPolicyMsg, opt.TxOpts)
}

// DeleteObjectPolicy delete the object policy of the principal
func (c *client) DeleteObjectPolicy(ctx context.Context, bucketName, objectName string, principalStr types.Principal, opt types.DeletePolicyOption) (string, error) {
	principal := &permTypes.Principal{}
	if err := principal.Unmarshal([]byte(principalStr)); err != nil {
		return "", err
	}

	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)
	return c.sendDelPolicyTxn(ctx, c.MustGetDefaultAccount().GetAddress(), resource.String(), principal, opt.TxOpts)
}

// IsObjectPermissionAllowed check if the permission of the object is allowed to the user
func (c *client) IsObjectPermissionAllowed(ctx context.Context, userAddr string,
	bucketName, objectName string, action permTypes.ActionType,
) (permTypes.Effect, error) {
	_, err := sdk.AccAddressFromHexUnsafe(userAddr)
	if err != nil {
		return permTypes.EFFECT_DENY, err
	}
	verifyReq := storageTypes.QueryVerifyPermissionRequest{
		Operator:   userAddr,
		BucketName: bucketName,
		ObjectName: objectName,
		ActionType: action,
	}

	verifyResp, err := c.chainClient.VerifyPermission(ctx, &verifyReq)
	if err != nil {
		return permTypes.EFFECT_DENY, err
	}

	return verifyResp.Effect, nil
}

// GetObjectPolicy get the object policy info of the user specified by principalAddr
func (c *client) GetObjectPolicy(ctx context.Context, bucketName, objectName string, principalAddr string) (*permTypes.Policy, error) {
	_, err := sdk.AccAddressFromHexUnsafe(principalAddr)
	if err != nil {
		return nil, err
	}

	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)
	queryPolicy := storageTypes.QueryPolicyForAccountRequest{
		Resource:         resource.String(),
		PrincipalAddress: principalAddr,
	}

	queryPolicyResp, err := c.chainClient.QueryPolicyForAccount(ctx, &queryPolicy)
	if err != nil {
		return nil, err
	}

	return queryPolicyResp.Policy, nil
}

// ListObjects return object list of the specific bucket
func (c *client) ListObjects(ctx context.Context, bucketName string, opts types.ListObjectsOptions) (types.ListObjectsResult, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return types.ListObjectsResult{}, err
	}

	const listObjectsDefaultMaxKeys = 50
	if opts.MaxKeys == 0 {
		opts.MaxKeys = listObjectsDefaultMaxKeys
	}

	if opts.StartAfter != "" {
		if err := s3util.CheckValidObjectName(opts.StartAfter); err != nil {
			return types.ListObjectsResult{}, err
		}
	}

	if opts.ContinuationToken != "" {
		decodedContinuationToken, err := base64.StdEncoding.DecodeString(opts.ContinuationToken)
		if err != nil {
			return types.ListObjectsResult{}, err
		}
		opts.ContinuationToken = string(decodedContinuationToken)

		if err = s3util.CheckValidObjectName(opts.ContinuationToken); err != nil {
			return types.ListObjectsResult{}, err
		}

		if !strings.HasPrefix(opts.ContinuationToken, opts.Prefix) {
			return types.ListObjectsResult{}, fmt.Errorf("continuation-token does not match the input prefix")
		}
	}

	if ok := utils.IsValidObjectPrefix(opts.Prefix); !ok {
		return types.ListObjectsResult{}, fmt.Errorf("invalid object prefix")
	}

	params := url.Values{}
	params.Set("max-keys", strconv.FormatUint(opts.MaxKeys, 10))
	params.Set("start-after", opts.StartAfter)
	params.Set("continuation-token", opts.ContinuationToken)
	params.Set("delimiter", opts.Delimiter)
	params.Set("prefix", opts.Prefix)
	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		contentSHA256: types.EmptyStringSHA256,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return types.ListObjectsResult{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return types.ListObjectsResult{}, err
	}
	defer utils.CloseResponse(resp)

	// unmarshal the json content from response body
	buf := new(strings.Builder)
	_, err = io.Copy(buf, resp.Body)
	if err != nil {
		log.Error().Msg("the list of objects in user's bucket:" + bucketName + " failed: " + err.Error())
		return types.ListObjectsResult{}, err
	}

	listObjectsResult := types.ListObjectsResult{}
	bufStr := buf.String()
	err = json.Unmarshal([]byte(bufStr), &listObjectsResult)
	// TODO(annie) remove tolerance for unmarshal err after structs got stabilized
	if err != nil && listObjectsResult.Objects == nil {
		log.Error().Msg("the list of objects in user's bucket:" + bucketName + " failed: " + err.Error())
		return types.ListObjectsResult{}, err
	}

	if opts.ShowRemovedObject {
		return listObjectsResult, nil
	}

	// default only return the object that has not been removed
	objectMetaList := make([]*types.ObjectMeta, 0)
	for _, objectInfo := range listObjectsResult.Objects {
		if objectInfo.Removed {
			continue
		}

		objectMetaList = append(objectMetaList, objectInfo)
	}

	listObjectsResult.Objects = objectMetaList
	listObjectsResult.KeyCount = strconv.Itoa(len(objectMetaList))
	return listObjectsResult, nil
}

// GetCreateObjectApproval returns the signature info for the approval of preCreating resources
func (c *client) GetCreateObjectApproval(ctx context.Context, createObjectMsg *storageTypes.MsgCreateObject) (*storageTypes.MsgCreateObject, error) {
	unsignedBytes := createObjectMsg.GetSignBytes()

	// set the action type
	urlValues := url.Values{
		"action": {types.CreateObjectAction},
	}

	reqMeta := requestMeta{
		urlValues:     urlValues,
		urlRelPath:    "get-approval",
		contentSHA256: types.EmptyStringSHA256,
		txnMsg:        hex.EncodeToString(unsignedBytes),
	}

	sendOpt := sendOptions{
		method:     http.MethodGet,
		isAdminApi: true,
	}

	bucketName := createObjectMsg.BucketName
	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return nil, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return nil, err
	}

	// fetch primary signed msg from sp response
	signedRawMsg := resp.Header.Get(types.HTTPHeaderSignedMsg)
	if signedRawMsg == "" {
		return nil, errors.New("fail to fetch pre createObject signature")
	}

	signedMsgBytes, err := hex.DecodeString(signedRawMsg)
	if err != nil {
		return nil, err
	}

	var signedMsg storageTypes.MsgCreateObject
	storageTypes.ModuleCdc.MustUnmarshalJSON(signedMsgBytes, &signedMsg)

	return &signedMsg, nil
}

// CreateFolder send create empty object txn to greenfield chain
func (c *client) CreateFolder(ctx context.Context, bucketName, objectName string, opts types.CreateObjectOptions) (string, error) {
	if !strings.HasSuffix(objectName, "/") {
		return "", errors.New("failed to create folder. Folder names must end with a forward slash (/) character")
	}

	reader := bytes.NewReader([]byte(``))
	txHash, err := c.CreateObject(ctx, bucketName, objectName, reader, opts)
	return txHash, err
}

// GetObjectUploadProgress return the status of object including the uploading progress
func (c *client) GetObjectUploadProgress(ctx context.Context, bucketName, objectName string) (string, error) {
	status, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return "", err
	}

	// get object status from sp
	if status.ObjectStatus == storageTypes.OBJECT_STATUS_CREATED {
		uploadProgressInfo, err := c.getObjectStatusFromSP(ctx, bucketName, objectName)
		if err != nil {
			return "", errors.New("fail to fetch object uploading progress from sp" + err.Error())
		}
		return uploadProgressInfo.ProgressDescription, nil
	}

	return status.ObjectStatus.String(), nil
}

// GetObjectResumableUploadOffset return the status of object including the uploading progress
func (c *client) GetObjectResumableUploadOffset(ctx context.Context, bucketName, objectName string) (uint64, error) {
	status, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return 0, err
	}

	// get object status from sp
	if status.ObjectStatus == storageTypes.OBJECT_STATUS_CREATED {
		uploadOffsetInfo, err := c.getObjectOffsetFromSP(ctx, bucketName, objectName)
		if err != nil {
			return 0, errors.New("fail to fetch object uploading offset from sp" + err.Error())
		}
		return uploadOffsetInfo.Offset, nil
	}

	// TODO(chris): may error
	return 0, nil
}

func (c *client) getObjectOffsetFromSP(ctx context.Context, bucketName, objectName string) (types.UploadOffset, error) {
	params := url.Values{}
	params.Set("upload-context", "")

	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		return types.UploadOffset{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)

	if err != nil {
		// not exist
		if find := strings.Contains(err.Error(), "no uploading record"); find {
			return types.UploadOffset{Offset: 0}, nil
		} else {
			return types.UploadOffset{}, err
		}
	}

	defer utils.CloseResponse(resp)

	objectOffset := types.UploadOffset{}
	// decode the xml content from response body
	err = xml.NewDecoder(resp.Body).Decode(&objectOffset)
	if err != nil {
		return types.UploadOffset{}, err
	}

	return objectOffset, nil
}

func (c *client) getObjectStatusFromSP(ctx context.Context, bucketName, objectName string) (types.UploadProgress, error) {
	params := url.Values{}
	params.Set("upload-progress", "")

	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		return types.UploadProgress{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return types.UploadProgress{}, err
	}

	defer utils.CloseResponse(resp)

	objectStatus := types.UploadProgress{}
	// decode the xml content from response body
	err = xml.NewDecoder(resp.Body).Decode(&objectStatus)
	if err != nil {
		return types.UploadProgress{}, err
	}

	return objectStatus, nil
}

func (c *client) UpdateObjectVisibility(ctx context.Context, bucketName, objectName string,
	visibility storageTypes.VisibilityType, opt types.UpdateObjectOption) (string, error) {
	objectInfo, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return "", fmt.Errorf("object:%s not exists: %s\n", objectName, err.Error())
	}

	if objectInfo.GetVisibility() == visibility {
		return "", fmt.Errorf("the visibility of object:%s is already %s \n", objectName, visibility.String())
	}

	updateObjectMsg := storageTypes.NewMsgUpdateObjectInfo(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName, visibility)

	// set the default txn broadcast mode as sync mode
	if opt.TxOpts == nil {
		broadcastMode := tx.BroadcastMode_BROADCAST_MODE_SYNC
		opt.TxOpts = &gnfdsdk.TxOption{Mode: &broadcastMode}
	}

	return c.sendTxn(ctx, updateObjectMsg, opt.TxOpts)
}
